{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 — Linear Regression 2 (CKD)\n\nThis notebook applies Linear Regression (OLS, Ridge, Lasso, ElasticNet) to the CKD dataset and explores degree-2 polynomial features.\n\n**Target:** `SerumCreatinine` (change if needed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Setup & Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\nfrom sklearn.model_selection import train_test_split, cross_validate, KFold\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport matplotlib.pyplot as plt\n\n# If running locally, put the CSV in the same folder as this notebook\nDATA_PATH = 'Chronic_Kidney_Dsease_data.csv'\nTARGET_COL = 'SerumCreatinine'  # change if needed\n\n# Load\ndf = pd.read_csv(DATA_PATH)\ndf.columns = [str(c).strip() for c in df.columns]\n\ndef coerce_numeric(s):\n    return pd.to_numeric(s.replace({'?': np.nan, 'NA': np.nan, 'None': np.nan, 'na': np.nan, '': np.nan}), errors='coerce')\n\n# Convert object columns that are mostly numeric-looking\nfor c in df.columns:\n    if df[c].dtype == object:\n        z = coerce_numeric(df[c])\n        if np.isfinite(z).mean() > 0.6:\n            df[c] = z\n\n# Split features/target\nassert TARGET_COL in df.columns, f\"'{TARGET_COL}' not found in dataframe.\"\nX = df.drop(columns=[TARGET_COL])\ny = df[TARGET_COL]\n\n# Column typing\nnum_cols = [c for c in X.columns if np.issubdtype(X[c].dtype, np.number)]\ncat_cols = [c for c in X.columns if c not in num_cols]\n\n# Preprocessing\nnumeric_pre = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler()),\n])\n\ncategorical_pre = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('ohe', OneHotEncoder(handle_unknown='ignore')),\n])\n\npre = ColumnTransformer([\n    ('num', numeric_pre, num_cols),\n    ('cat', categorical_pre, cat_cols),\n])\n\n# CV & metrics\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\nscoring = {\n    'rmse': 'neg_root_mean_squared_error',\n    'mae' : 'neg_mean_absolute_error',\n    'r2'  : 'r2'\n}\n\nprint('Shape:', df.shape)\ndf.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Baselines: OLS, Ridge, Lasso, ElasticNet (5-fold CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n\nresults = {}\n\ndef eval_model(name, model):\n    pipe = Pipeline([('pre', pre), ('model', model)])\n    cvres = cross_validate(pipe, X, y, scoring=scoring, cv=cv, return_train_score=False)\n    results[name] = {\n        'RMSE_mean': -cvres['test_rmse'].mean(),\n        'RMSE_std' :  cvres['test_rmse'].std(),\n        'MAE_mean' : -cvres['test_mae'].mean(),\n        'MAE_std'  :  cvres['test_mae'].std(),\n        'R2_mean'  :  cvres['test_r2'].mean(),\n        'R2_std'   :  cvres['test_r2'].std(),\n    }\n\nridge_alphas = np.logspace(-3, 3, 25)\n\neval_model('LinearRegression', LinearRegression())\neval_model('RidgeCV', RidgeCV(alphas=ridge_alphas))\neval_model('LassoCV', LassoCV(cv=cv, random_state=42, max_iter=10000))\neval_model('ElasticNetCV', ElasticNetCV(l1_ratio=[.2,.4,.6,.8,.9,.95,.99,1.0], cv=cv, random_state=42, max_iter=10000))\n\nbaseline_df = pd.DataFrame(results).T.sort_values('RMSE_mean')\nbaseline_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Polynomial Features (degree 2, numeric only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_poly = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler()),\n    ('poly', PolynomialFeatures(degree=2, include_bias=False))\n])\n\npre_poly = ColumnTransformer([\n    ('num', numeric_poly, num_cols),\n    ('cat', categorical_pre, cat_cols),\n])\n\npoly_results = {}\n\ndef eval_model_poly(name, model):\n    pipe = Pipeline([('pre', pre_poly), ('model', model)])\n    cvres = cross_validate(pipe, X, y, scoring=scoring, cv=cv, return_train_score=False)\n    poly_results[name] = {\n        'RMSE_mean': -cvres['test_rmse'].mean(),\n        'RMSE_std' :  cvres['test_rmse'].std(),\n        'MAE_mean' : -cvres['test_mae'].mean(),\n        'MAE_std'  :  cvres['test_mae'].std(),\n        'R2_mean'  :  cvres['test_r2'].mean(),\n        'R2_std'   :  cvres['test_r2'].std(),\n    }\n\n# Run a few representative polynomial variants\neval_model_poly('LinearRegression+Poly2', LinearRegression())\neval_model_poly('RidgeCV+Poly2', RidgeCV(alphas=ridge_alphas))\neval_model_poly('LassoCV+Poly2', LassoCV(cv=cv, random_state=42, max_iter=10000))\neval_model_poly('ElasticNetCV+Poly2', ElasticNetCV(l1_ratio=[.2,.4,.6,.8,.9,.95,.99,1.0], cv=cv, random_state=42, max_iter=10000))\n\npoly_df = pd.DataFrame(poly_results).T.sort_values('RMSE_mean')\npoly_df.round(4)\n\ncombined = pd.concat([baseline_df, poly_df]).sort_values('RMSE_mean')\ncombined.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Holdout & Residual Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = combined.index[0]\nuse_poly = 'Poly2' in best_name\n\nif use_poly:\n    use_pre = pre_poly\nelse:\n    use_pre = pre\n\nif 'Ridge' in best_name:\n    best_est = RidgeCV(alphas=ridge_alphas)\nelif 'Lasso' in best_name:\n    best_est = LassoCV(cv=cv, random_state=42, max_iter=10000)\nelif 'ElasticNet' in best_name:\n    best_est = ElasticNetCV(l1_ratio=[.2,.4,.6,.8,.9,.95,.99,1.0], cv=cv, random_state=42, max_iter=10000)\nelse:\n    best_est = LinearRegression()\n\nX_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=7)\npipe_best = Pipeline([('pre', use_pre), ('model', best_est)]).fit(X_tr, y_tr)\ny_hat = pipe_best.predict(X_te)\n\nrmse = mean_squared_error(y_te, y_hat, squared=False)\nmae  = mean_absolute_error(y_te, y_hat)\nr2   = r2_score(y_te, y_hat)\n\nprint('Best by CV:', best_name)\nprint(f'Holdout RMSE: {rmse:.4f}')\nprint(f'Holdout MAE : {mae:.4f}')\nprint(f'Holdout R^2 : {r2:.4f}')\n\n# Residuals vs Fitted\nresid = y_te - y_hat\nplt.figure()\nplt.scatter(y_hat, resid, alpha=0.7)\nplt.axhline(0, linestyle='--')\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')\nplt.title('Residuals vs Fitted — ' + best_name)\nplt.show()\n\n# Simple Q–Q without seaborn/scipy\nres_sorted = np.sort(resid)\nn = len(res_sorted)\nprobs = (np.arange(1, n+1) - 0.5) / n\ntheoretical = np.sqrt(2) * np.erfinv(2*probs - 1)\nz_res = (res_sorted - res_sorted.mean()) / res_sorted.std(ddof=1)\n\nplt.figure()\nplt.scatter(theoretical, z_res, alpha=0.7)\nmn, mx = theoretical.min(), theoretical.max()\nplt.plot([mn, mx], [mn, mx], linestyle='--')\nplt.xlabel('Theoretical Quantiles (Z)')\nplt.ylabel('Standardized Residual Quantiles')\nplt.title('Q–Q Plot — ' + best_name)\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Notes (edit this block)\n\n- **Best model (by CV RMSE):** _(fill from `combined` table)_  \n- **Holdout metrics:** RMSE≈…, MAE≈…, R²≈…  \n- **Polynomial features:** Helped/hurt depending on interaction strength; regularization controlled variance.  \n- **Assumptions:** Residuals vs fitted suggest _[approx/non]_ constant variance; Q–Q suggests _[approx/non]_ normality.  \n- **Next steps:** Try log transforms for skewed labs, targeted interactions, and compare to tree-based models in Week 6.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
